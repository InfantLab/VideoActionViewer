# Multimodal Video Annotation Viewer Specification

## Overview and Motivation

This specification describes a web-based **Video Annotation Viewer** for reviewing offline-processed videos along with rich multimodal annotations. The goal is to provide an intuitive interface to **quickly navigate to interesting moments** in the video while clearly displaying various annotations (visual, audio, and textual). Unlike heavy-duty tools like CVAT or clunky interfaces like LabelStudio, this custom viewer focuses on **simplicity and efficiency** for the specific use case of human interaction analysis with synchronized audio-visual cues. Existing tools such as DIVE are geared mostly toward generic object detection/tracking and lack integrated audio or multi-person interaction context, hence a purpose-built viewer is warranted. This first version is **read-only** (for viewing/inspection only – no editing or uploading of new data), but the design will lay the groundwork for future annotation validation and editing capabilities.

**Key objectives:**

* **Multimodal Overlay Toggles:** Allow the user to selectively overlay different analysis results on the video (human poses, facial emotion indicators, audio emotion, subtitles, and detected events).
* **Rich Timeline Visualization:** Display a synchronized timeline below the video that shows annotation events and continuous signals (e.g. motion intensity, audio waveform) at a glance, enabling easy scanning and scrubbing.
* **Interactive Navigation:** Facilitate quick navigation via the timeline and controls – the user can jump to moments of interest (e.g. peaks in motion/audio or specific labeled events) and see all relevant context at that time.
* **Performance and Usability:** Ensure smooth video playback with frame-synchronized drawing of annotations (using efficient rendering techniques). Keep the UI uncluttered and intuitive, suitable for researchers validating algorithm outputs.

## Video Player & Overlay Annotations

The core of the viewer is an HTML5 video player component augmented with custom overlay layers for annotations. The video player should support standard functions (play, pause, seek, etc.) along with the ability to toggle various annotation overlays on and off. When an overlay type is enabled, the corresponding annotations for the current video frame are drawn on top of the video in real time, synchronized to playback.

* **Pose Overlay:** When enabled, the viewer draws human pose skeletons on the video for each detected person in the frame. This consists of keypoints (e.g. joints) connected by lines to illustrate body posture. The pose data comes as frame-by-frame keypoint coordinates in the JSON. The overlay should update every frame to reflect movements. If multiple people are present, the system should distinguish their poses (e.g. color-code each person or label them with IDs) so that interactions are clear.

* **Facial Emotion Overlay:** Toggling this on will display indicators of detected facial emotions for people in the frame. For example, if the analysis provides a bounding box around a face and an emotion label (like *happy, sad, surprised*), the viewer can draw the face box and either color-code it or put a small text label or icon (emoji) near the face to denote the emotion. This overlay updates per frame (or per interval) as emotions change. As with poses, multiple individuals’ emotions can be shown simultaneously (each near the respective face). The design should strive to keep these labels unobtrusive yet noticeable (small text or icons, possibly with a semi-transparent background).

* **Audio Emotion Overlay:** Because audio-based emotion is not tied to a specific visible object, this overlay can be handled slightly differently. When enabled, the viewer can display the current **vocal emotion** state on screen (for example, as text like “Voice Emotion: Angry” or an emoji/icon) that updates as the audio mood changes over time. This could be placed in a corner of the video or above the subtitles area. If the audio emotion analysis is segment-based (e.g. each spoken utterance is labeled with an emotion), the overlay text/icon should appear only during those segments. The key is to give a visual cue of the tone of the audio to complement the visuals. (If multiple speakers’ emotions are distinguished in the data, a more complex UI might be needed – but for now we assume a single overall audio emotion track.)

* **Subtitles Overlay:** When toggled on, the viewer should display subtitles (transcribed speech) at the bottom of the video frame, similar to typical video subtitles. These come from the JSON’s event data (with *start time, end time, and text* for each subtitle). The subtitles should appear and disappear at the correct times, and use a readable font with a contrast outline for visibility. The user can turn this off if they find the text distracting, but it is crucial for analyzing speech content alongside the video.

* **Event Overlay:** This covers other discrete events detected in the video, such as actions or interactions (e.g. *“hugging”*, *“pointing”*, *“child looks at parent”*). When enabled, if an event is occurring at the current time, the viewer can show a brief label on the video to indicate it. For example, a text label like “Event: Hugging” could be shown at the top of the video or near the relevant subjects. In cases where the event is tied to a specific person or object (if that info is available), the overlay might highlight that person (perhaps by a different colored outline on their pose or a marker) to draw attention. For more general events, a simple on-screen text notification or icon during the event’s duration suffices.

All these overlays should be **synchronized with the video playback**. We cannot rely on pre-rendered burned-in annotations; instead the app must dynamically draw them at runtime based on the current frame/time. A typical approach is to overlay a HTML5 `<canvas>` or SVG on top of the video element to draw shapes (boxes, lines, text) each frame. This allows flexible enabling/disabling of specific annotations without needing to reload or modify the video file. The viewer should hide the default browser video controls and provide its own controls (see below) because we need to hook into the playback loop to update overlays every frame. Using `requestAnimationFrame` or the video’s timeupdate events, the app can trigger redraws of the canvas with only the overlays that are toggled on. This ensures minimal overhead when many overlays are turned off, and smooth visuals when they are on.

**Playback Controls:** Standard video controls must be provided in a user-friendly way, likely in a custom control bar below the video (or integrated with the timeline). At minimum, include: a Play/Pause button, a current time and total duration display, and a seek slider or the ability to click the timeline to seek. Additionally, a **frame-step** button (to go to the next or previous frame) and a playback speed control (e.g. 0.5×, 1×, 2×) would be very useful for close examination of events and faster scanning through footage. A volume control (mute/unmute or slider) should also be included so that users can listen to audio or turn it off as needed. Since we have a custom timeline (see next section), seeking will primarily be handled through that timeline interface or by dragging the playhead. We should also allow the user to toggle fullscreen mode for the video player, so they can enlarge the video when needed (the overlays should scale appropriately in fullscreen).

The overlay toggle UI can be a set of checkboxes or toggle switches labeled **Pose, Face Emo., Audio Emo., Subtitles, Events**. These could be located in a small panel either above the video or to the side. When a toggle is off, the corresponding overlay layer is hidden (and no drawing occurs for it); when on, the overlay is active. This real-time control lets users declutter the view as needed – for example, they might turn off pose and face overlays to focus on subtitles and audio sentiment at some point, or vice versa. The system should handle any combination of toggles gracefully. (If many overlays are on, ensure the canvas draws all of them each frame.) This dynamic control is a key requirement – as noted in a relevant case study, relying on static baked-in annotations isn’t feasible for interactive exploration; instead, the client must be able to **highlight specific annotations or show/hide them on the fly**.

## Timeline Visualization & Navigation

Immediately below the video player, the application will feature a **rich timeline** that provides an abstract overview of all temporal annotations and signals. This timeline is crucial for quickly scanning the video’s length and identifying segments of interest (e.g. periods of high activity or notable events). It combines discrete event annotations (like subtitles and labeled events) with continuous time-series plots (like motion and audio amplitude).

&#x20;*Example of a timeline view in a video annotation tool’s interface. The video playback area is at the top, and the timeline below shows annotated segments as blue bars with a vertical playhead marker (black line) indicating the current frame. In this example (VideoAnnotator), there are also playback controls beneath the timeline and additional panels for editing metadata. Our viewer will employ a similar layout with the timeline directly under the video, though focused purely on visualization since editing is out of scope.*

**Timeline contents:** The timeline will be a horizontally scrollable (or zoomable) view spanning from 0 to the video’s total duration. It should include the following elements, likely stacked as separate tracks or layers for clarity:

* **Current Time Marker:** A vertical line or pointer that indicates the current playback time on the timeline. This marker moves along the timeline as the video plays. Users can also grab and drag this marker to **seek** through the video (scrubbing). As they drag, the video frame should update accordingly (with a small delay or after drop if needed for performance). The current timestamp should be displayed on or near the marker (for precision).

* **Subtitle Track:** A track that displays subtitle segments along the timeline. Each subtitle event (from the annotations JSON) can be represented as a colored bar or box spanning its start to end time. The subtitle text could be displayed within the bar or as a tooltip on hover (to avoid overly long text on the timeline). For example, a subtitle from 10.0s to 12.0s would appear as a bar covering that interval. If subtitle text is short, we might write it on the bar; if not, just show a small marker or a generic “speech” icon and let the user hover to see the full text. This track gives a visual indication of where speech occurs and roughly how long each utterance lasts. It should align with the video time axis (e.g., if subtitles are sparse, you’ll see gaps; if continuous conversation, the bars will almost touch).

* **Event Track(s):** One or more tracks showing other **event-based annotations** (besides subtitles). This could include action detection events (e.g. *“Hugging”*, *“Clapping”*, *“Looking at toy”*, etc.) or any labeled interaction. Each event has a start and end time, and possibly a category. We can represent each event as a bar or block on the timeline, similar to subtitles. If there are different types of events, we might use **color-coding or separate rows** to distinguish them. For instance, all “interaction events” could be on one track and “special events” on another, or each major category gets its own row for clarity. Alternatively, a single combined event track could be used with colored bars (and a legend). The design choice can be refined based on how many distinct event types exist. In any case, the event segments should be clearly visible (maybe slightly taller than subtitle bars, or with distinct colors) so that users can quickly see where these notable events occur. Like subtitles, hovering or clicking an event bar could show a tooltip with details (e.g. “Hugging (Person A and B)” if such detail exists).

* **Audio Waveform Track:** A visualization of the audio amplitude over time (the waveform). This provides a quick insight into where there is speech or sound (high amplitude) versus silence or low activity. We can generate this by processing the video’s audio track (or possibly using precomputed data from the processing pipeline if provided). The waveform can be drawn as a filled area chart or line plot that mirrors the audio volume. Time is on the X-axis (aligned with other tracks), and amplitude on Y (though we may not label the Y axis in absolute terms; it’s just relative loudness). This track should be vertically short but enough to see spikes. It could appear immediately below the discrete event tracks. If the analysis includes **audio emotion segments** (e.g. segments labeled with emotions), we might overlay or underline those on the waveform – for example, color the waveform region in a certain color when the speaker is angry vs calm. Alternatively, we could dedicate an **Audio Emotion track**: e.g., a thin bar above or below the waveform that changes color or label according to emotion over time. This depends on data granularity – if the audio emotion is effectively an event per utterance, it might be easier to treat it like an event track (with segments labeled “Angry voice”, “Happy voice”, etc., aligning with spoken segments). For now, we will assume we have either continuous values or segment labels for audio emotion and plan to represent it visually in tandem with the waveform (perhaps as a colored overlay or an adjacent track).

* **Average Motion Track:** A plot showing the **motion intensity** over time. This could be a single line graph or area (with time on X and some measure of motion on Y). “Average motion” might be computed by the pipeline per frame (e.g. average optical flow magnitude, or amount of movement in the scene). High peaks would correspond to a lot of movement (people moving energetically) and flat low areas to stillness. Plotting this helps identify energetic moments (which might coincide with interesting interactions). This track can sit alongside the waveform track in terms of height. Possibly we could overlay it on the waveform plot if scaled, but that could be confusing; better to have it separate or visually distinct (e.g. a line in a different color sharing the same space if needed, with a secondary axis). Simpler is to have a dedicated track row for motion intensity, directly comparable in time to waveform above it.

All timeline elements share the same timeline axis and current-time marker. The timeline should have time labels/ticks along the top or bottom for orientation (e.g. marking seconds or minutes). If the video is very long, a zoom/pan feature or a mini-map (overview) can be provided to navigate efficiently. For instance, the user might zoom out to see the entire timeline as a summary (possibly with a heatmap of annotation density), then zoom in to inspect a particular segment. However, for the initial version, a simple full-width timeline with maybe horizontal scrolling if needed is acceptable.

**Interactivity:** The timeline is not just a static display – it’s a key navigation tool:

* **Click & Drag to Seek:** The user can click anywhere on the timeline or on a specific annotation bar to jump the video to that time. For example, clicking on a subtitle segment will move the video to the start of that subtitle (or perhaps the exact clicked time if in the middle). The vertical playhead marker will reposition accordingly, and the video frame and overlays will update to that moment. Users can also drag the playhead smoothly to scrub through the video; as they drag, a preview of the time (and possibly frame) is shown. Once released (or continuously, if performance permits), the video seeks to that frame. This direct manipulation of the timeline allows pinpoint navigation far more efficiently than trying to drag a generic progress slider.

* **Toggling Tracks:** The same toggles that control video overlays can also affect the timeline display. For consistency, if a user turns off “Subtitles” overlay, the timeline’s subtitle track could optionally hide as well (since they might not be interested in those at the moment). Similarly, toggling “Events” could hide the event track. This can reduce visual clutter in the timeline if the user is momentarily focusing on one modality (e.g. just audio and motion). We could also provide separate show/hide controls for timeline tracks if needed (for example, maybe audio waveform and motion graph toggles), but it might be acceptable to always show waveform and motion since they’re lightweight. The main point is to let users customize the timeline view to their needs.

* **Segment Highlight & Tooltips:** When the user hovers over an event bar or subtitle bar on the timeline, it could highlight that segment (e.g. brighten it or show a border) and display a tooltip with details (such as the full subtitle text or the event type label, and exact timestamps). This helps in quickly previewing what an event is before clicking. For example, hovering over a small event bar might pop up “Action: High Five (12.3s – 13.1s)”. For audio waveform/motion, a tooltip could show the time and value when hovering on the graph, though that’s less crucial.

* **Quick Skip Buttons:** In addition to timeline clicking, we might include UI buttons to jump to the **next or previous annotation**. For instance, “Next Event” or “Next Subtitle” buttons can move the playhead to the start of the next segment of interest. This is useful if a user wants to skip through non-annotated parts quickly. Implementing this would involve scanning the timeline annotation list for the next start time after the current time. (This is a nice-to-have feature for faster navigation; if not in v1, the timeline clicking itself might suffice.)

Importantly, **the timeline in this version is read-only** – users cannot alter annotations from here (no dragging event edges or creating new segments). It’s purely for visualization and navigation, similar to a “review mode”. Future versions for validation might allow edits (e.g. adjusting segment times), so the design should keep that in mind (for example, the timeline could later support draggable handles on event bars, etc.). But for now, to avoid confusion, interactions that would modify data are disabled. This matches the notion of a *read-only summary timeline* as used in some tools to prevent accidental changes while reviewing.

## Data Format and Handling

The viewer will consume annotation data provided in a JSON file (or equivalent structured format). The JSON is assumed to contain two main types of information: **time-series frame data** and **event segment data**. The architecture should be flexible to accommodate the exact schema, but we expect something along these lines:

* **Frame-by-frame data:** For each video frame (or each timestamp interval), there may be data such as pose keypoints, facial attributes, and possibly per-frame emotion scores. This could be structured, for example, as a list or dictionary keyed by frame number or timestamp. For instance: `"frames": { "0": { "persons": [ { "id": 1, "pose": {...}, "face": {...} }, {...} ] }, "1": {...}, ... }`. Each frame entry might include multiple persons’ data. Pose data could be a list of keypoint coordinates; face data might include bounding box and an emotion label or confidence. Audio-based data might also appear here if it’s computed per frame or short window (e.g. an “audio\_emotion\_score” for that frame). The viewer will use this data when drawing overlays: e.g. at frame N, iterate through each person in `frames[N]` and draw their pose lines and face label if toggled on.

* **Event data:** A list of events, where each event has at least a type (or category), a start time, end time, and possibly additional info (like a label or content). Subtitles, for example, could be events of type “subtitle” with a text field. Action or interaction events might have a label field describing the event (and possibly an `actor` or `participants` field referencing person IDs if applicable). Audio emotion might be given as events too – e.g., segments of speech with an emotion classification. An example event entry: `{ "type": "action", "label": "hugging", "start": 10.5, "end": 12.0, "persons": [1,2] }` or a subtitle: `{ "type": "subtitle", "content": "Hello there!", "start": 5.0, "end": 7.0 }`. The viewer should parse this list and group events by type for display. This data feeds the timeline (creating bars for each event) and also the on-video overlays (showing text during the event if the respective overlay is on).

The JSON will be loaded when a video is opened. We assume the video file itself can be either loaded by the user (e.g. via a file input in the app) or pre-configured. In this version, we **do not implement an upload pipeline** or processing – we assume the user already has the video and its matching JSON (perhaps with the same base filename) and can provide them to the viewer. A simple file-picker for the JSON and video could be offered at app start (for example, “Open Video” and “Open Annotation File” buttons). Once loaded, the app will parse the JSON into appropriate data structures (arrays, dictionaries) in memory for quick access during playback. If the JSON is large (e.g. very long videos), we should ensure that parsing is efficient. We might not need to hold *every* frame’s data in memory if huge; since it’s offline data, it’s okay, but we could also lazy-load or stream if needed. Given typical use (maybe videos of a few minutes to an hour), a JSON with per-frame data (at 30 fps, 1800 frames per minute) is manageable.

One consideration is ensuring **synchronization** between video time and the annotations. If JSON uses frame indices, we need the video’s frame rate or a mapping from time to frame. If JSON uses timestamps (seconds), we rely on those. HTML5 video operates on time in seconds (floating point), which is fine. But if we draw pose by frame, we might use the frame timestamp to retrieve the nearest frame data. It would help to have the video’s frame rate or duration in the metadata. The pipeline could even embed the frame rate in JSON. For precise overlay, one approach is to use the video’s current time (`video.currentTime`) and compute the corresponding frame index = currentTime \* frameRate (rounded or floored). However, slight timing differences can occur. Using the *exact same frame timestamps* as the pipeline is ideal. If provided, we use those. Otherwise, an assumption of constant 30 FPS or similar could be made. We will also take into account if the video’s fps is different; aligning those as DIVE does (downsample or bucket frames) might be necessary. In summary, the implementation will need to carefully match the annotation data to the video’s progression. For now, we assume a straightforward scenario where either frame numbers align 1:1 with video frames or timestamps are used directly (the JSON can indicate how to interpret it).

The viewer application will likely be built in JavaScript/TypeScript (since we are using Lovable, which can generate a full-stack app, this will be primarily a front-end feature). It can run entirely in the browser – loading the video (via an `<video>` tag) and reading the JSON (via File API or fetch if hosted). No server-side component is strictly needed for this viewing functionality. This makes it easy to deploy (just a static web app where the user can select their local files or we host sample ones).

To implement drawing of overlays efficiently, the use of the `<canvas>` element overlaying the video is recommended. Each animation frame, we clear the canvas and draw active annotations: e.g., draw lines for each pose (maybe using the canvas 2D context), draw rectangles for faces with tiny text, etc. Drawing the audio waveform and motion graph can be done on a separate canvas in the timeline area (or using an SVG/chart library). For example, we might precompute the waveform curve once (when loading the video, we can decode audio data or use a library to get amplitude per segment) and then draw it as an SVG polyline or canvas path. The motion data likely comes directly as an array of values per frame; we can downsample it for display if needed (if frame rate is high relative to pixel width). These graphs should update their highlight/marker as the playhead moves, but they themselves are static background of the timeline once rendered.

Finally, the data format should be flexible. We may want to support alternative inputs in the future (for example, multiple JSON files for different annotation sets, or streaming in annotations from a server). But the core assumption is a single JSON with everything. The app could be coded to **watch for mismatches** (e.g., if JSON timestamps exceed video duration, or vice versa) and handle gracefully (e.g., ignore out-of-range entries).

It is worth noting that storing video annotations in JSON is a common approach for easy integration with pipelines. We will follow this practice, treating the JSON as the source of truth for all overlays and timeline data. The video filename might be referenced in the JSON (or the JSON might be named similarly); the UI could try to auto-load the JSON if a user selects a video (for convenience, e.g., by looking for a matching name). But an explicit file input is simplest to start.

## Future Enhancements and Considerations

While this specification focuses on a **viewer-only version**, it’s designed with future expansion in mind:

* **Human Validation & Editing:** Future versions will allow users to correct or adjust annotations. The UI layout we chose (video over timeline, with distinct annotation tracks) will lend itself to adding interactive controls. For example, in a future release one could click and drag the edges of an event bar on the timeline to tweak its timing, or select a pose in a frame and reassign it. Our current implementation will not include these interactions (to avoid complexity), but the code structure (e.g., separating rendering logic from data, and having identifiable objects for each annotation) should make it feasible to add editing features. We’ve essentially created the “review mode” now; an “edit mode” could later be introduced with minimal disruption.

* **Scalability:** If the viewer needs to handle very long videos (tens of minutes or hours) or a huge number of annotations, performance will be a concern. Techniques like virtualized timeline (render only visible portion), level-of-detail rendering (e.g. not drawing every single frame’s detail when zoomed out), and efficient data structures (maybe indexing events by time for quick lookup when seeking) might be needed. The “timeline summary” concept (displaying a high-level heatmap of annotation density) could be useful for long videos to avoid visually overwhelming the user. Additionally, caching of drawn frames or using WebGL for drawing many pose keypoints could be considered if needed. For now, typical use (research videos likely a few minutes each with manageable annotations) should be fine with canvas 2D and straightforward loops.

* **Multi-Video or Comparison Views:** Currently, we view one video at a time. In future, users might want to compare two runs or view multi-camera angles. The interface could be extended to allow loading multiple videos side by side, each with its own synchronized annotations, or overlaying certain data from one video onto another’s timeline. This is outside our current scope, but the modular approach (distinct data and overlay layers for each source) would help if we ever go there.

* **Integration with Pipelines:** Eventually, the app could integrate an uploading mechanism and even run analysis (via backend or WebAssembly) so that a user can input a raw video and get annotations in the viewer. At that point, the spec would include managing jobs, showing progress, etc. For now, we explicitly exclude those parts – we assume the pipeline is external and has produced the JSON. However, making the app open-source and easily deployable means others could use it to visualize their own pipeline outputs by simply adhering to the JSON format.

* **Customization and Extensibility:** We should keep the code flexible to accept new annotation types. For example, if later we want to overlay gaze tracking or object bounding boxes, we can add another toggle and render logic without redesigning everything. Using a configuration or simply a well-organized rendering function per layer type will help. The timeline too can be generated from the data types present – e.g., if no subtitles in JSON, no subtitle track is shown. This way the viewer is not tied strictly to the five toggled categories, but can adapt (the UI can hide toggles for data that isn’t there).

In summary, this viewer will deliver an interactive, multimodal visualization platform for video analysis results. It emphasizes an **overlay-rich video player** and a **detailed timeline** that together allow the user to see *what* happened, *when* it happened, and to jump directly to those moments. By combining visual cues (poses, faces), textual cues (subtitles, labels), and quantitative cues (audio waveform, motion graph), the tool provides a comprehensive view for analyzing human interactions in the video. We leverage known best practices from existing annotation tools – e.g., video+canvas overlay for dynamic drawing, timeline with playhead and segments – and tailor them to our needs (including audio and emotion data which most CV annotation tools ignore). The end result will be a **web application** (built using the Lovable platform) that can be opened in a browser, allowing researchers or annotators to load their video and JSON, then efficiently review and validate the multimodal annotations.